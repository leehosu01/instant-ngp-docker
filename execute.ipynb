{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload your data\n",
    "---------------\n",
    "- upload single file to at path `/content`\n",
    "- If you try new example, delete old file\n",
    "\n",
    "### case 1, upload by mp4 (video)\n",
    "- upload `<mp4 file name>.mp4` at `/content`\n",
    "\n",
    "### case 2, upload by zip (images, compress multiple PNG or JPG)\n",
    "- upload `<zip file name>.zip` at `/content`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add instant-ngp to python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"instant-ngp\")\n",
    "sys.path.append(\"instant-ngp/build\")\n",
    "sys.path.append(\"instant-ngp/scripts\")\n",
    "import os\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colmap to nerf (MUST CHANGE FULL CODE BEFORE RELEASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def variance_of_laplacian(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "\n",
    "def sharpness(imagePath):\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = variance_of_laplacian(gray)\n",
    "    return fm\n",
    "\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array(\n",
    "        [\n",
    "            [\n",
    "                1 - 2 * qvec[2] ** 2 - 2 * qvec[3] ** 2,\n",
    "                2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "                2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2],\n",
    "            ],\n",
    "            [\n",
    "                2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "                1 - 2 * qvec[1] ** 2 - 2 * qvec[3] ** 2,\n",
    "                2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1],\n",
    "            ],\n",
    "            [\n",
    "                2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "                2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "                1 - 2 * qvec[1] ** 2 - 2 * qvec[2] ** 2,\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def rotmat(a, b):\n",
    "    a, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n",
    "    v = np.cross(a, b)\n",
    "    c = np.dot(a, b)\n",
    "    # handle exception for the opposite direction input\n",
    "    if c < -1 + 1e-10:\n",
    "        return rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n",
    "    s = np.linalg.norm(v)\n",
    "    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "    return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s**2 + 1e-10))\n",
    "\n",
    "\n",
    "def closest_point_2_lines(\n",
    "    oa, da, ob, db\n",
    "):  # returns point closest to both rays of form o+t*d, and a weight factor that goes to 0 if the lines are parallel\n",
    "    da = da / np.linalg.norm(da)\n",
    "    db = db / np.linalg.norm(db)\n",
    "    c = np.cross(da, db)\n",
    "    denom = np.linalg.norm(c) ** 2\n",
    "    t = ob - oa\n",
    "    ta = np.linalg.det([t, db, c]) / (denom + 1e-10)\n",
    "    tb = np.linalg.det([t, da, c]) / (denom + 1e-10)\n",
    "    if ta > 0:\n",
    "        ta = 0\n",
    "    if tb > 0:\n",
    "        tb = 0\n",
    "    return (oa + ta * da + ob + tb * db) * 0.5, denom\n",
    "\n",
    "\n",
    "def colmap2nerf(args):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"convert a text colmap export to nerf format transforms.json; optionally convert video to images, and optionally run colmap in the first place\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--video_in\",\n",
    "        default=\"\",\n",
    "        help=\"run ffmpeg first to convert a provided video file into a set of images. uses the video_fps parameter also\",\n",
    "    )\n",
    "    parser.add_argument(\"--video_fps\", default=2)\n",
    "    parser.add_argument(\n",
    "        \"--time_slice\",\n",
    "        default=\"\",\n",
    "        help=\"time (in seconds) in the format t1,t2 within which the images should be generated from the video. eg: \\\"--time_slice '10,300'\\\" will generate images only from 10th second to 300th second of the video\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--run_colmap\", action=\"store_true\", help=\"run colmap first on the image folder\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--colmap_matcher\",\n",
    "        default=\"sequential\",\n",
    "        choices=[\"exhaustive\", \"sequential\", \"spatial\", \"transitive\", \"vocab_tree\"],\n",
    "        help=\"select which matcher colmap should use. sequential for videos, exhaustive for adhoc images\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--colmap_db\", default=\"colmap.db\", help=\"colmap database filename\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--colmap_camera_model\",\n",
    "        default=\"OPENCV\",\n",
    "        choices=[\"SIMPLE_PINHOLE\", \"PINHOLE\", \"SIMPLE_RADIAL\", \"RADIAL\", \"OPENCV\"],\n",
    "        help=\"camera model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--colmap_camera_params\",\n",
    "        default=\"\",\n",
    "        help=\"intrinsic parameters, depending on the chosen model.  Format: fx,fy,cx,cy,dist\",\n",
    "    )\n",
    "    parser.add_argument(\"--images\", default=\"images\", help=\"input path to the images\")\n",
    "    parser.add_argument(\n",
    "        \"--text\",\n",
    "        default=\"colmap_text\",\n",
    "        help=\"input path to the colmap text files (set automatically if run_colmap is used)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--aabb_scale\",\n",
    "        default=16,\n",
    "        choices=[\"1\", \"2\", \"4\", \"8\", \"16\"],\n",
    "        help=\"large scene scale factor. 1=scene fits in unit cube; power of 2 up to 16\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_early\", default=0, help=\"skip this many images from the start\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_colmap_coords\",\n",
    "        action=\"store_true\",\n",
    "        help=\"keep transforms.json in COLMAP's original frame of reference (this will avoid reorienting and repositioning the scene for preview and rendering)\",\n",
    "    )\n",
    "    parser.add_argument(\"--out\", default=\"transforms.json\", help=\"output path\")\n",
    "    args = parser.parse_args(args.split())\n",
    "    AABB_SCALE = int(args.aabb_scale)\n",
    "    SKIP_EARLY = int(args.skip_early)\n",
    "    IMAGE_FOLDER = args.images\n",
    "    TEXT_FOLDER = args.text\n",
    "    OUT_PATH = args.out\n",
    "    print(f\"outputting to {OUT_PATH}...\")\n",
    "    with open(os.path.join(TEXT_FOLDER, \"cameras.txt\"), \"r\") as f:\n",
    "        angle_x = math.pi / 2\n",
    "        for line in f:\n",
    "            # 1 SIMPLE_RADIAL 2048 1536 1580.46 1024 768 0.0045691\n",
    "            # 1 OPENCV 3840 2160 3178.27 3182.09 1920 1080 0.159668 -0.231286 -0.00123982 0.00272224\n",
    "            # 1 RADIAL 1920 1080 1665.1 960 540 0.0672856 -0.0761443\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            els = line.split(\" \")\n",
    "            w = float(els[2])\n",
    "            h = float(els[3])\n",
    "            fl_x = float(els[4])\n",
    "            fl_y = float(els[4])\n",
    "            k1 = 0\n",
    "            k2 = 0\n",
    "            p1 = 0\n",
    "            p2 = 0\n",
    "            cx = w / 2\n",
    "            cy = h / 2\n",
    "            if els[1] == \"SIMPLE_PINHOLE\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "            elif els[1] == \"PINHOLE\":\n",
    "                fl_y = float(els[5])\n",
    "                cx = float(els[6])\n",
    "                cy = float(els[7])\n",
    "            elif els[1] == \"SIMPLE_RADIAL\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "                k1 = float(els[7])\n",
    "            elif els[1] == \"RADIAL\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "                k1 = float(els[7])\n",
    "                k2 = float(els[8])\n",
    "            elif els[1] == \"OPENCV\":\n",
    "                fl_y = float(els[5])\n",
    "                cx = float(els[6])\n",
    "                cy = float(els[7])\n",
    "                k1 = float(els[8])\n",
    "                k2 = float(els[9])\n",
    "                p1 = float(els[10])\n",
    "                p2 = float(els[11])\n",
    "            else:\n",
    "                print(\"unknown camera model \", els[1])\n",
    "            # fl = 0.5 * w / tan(0.5 * angle_x);\n",
    "            angle_x = math.atan(w / (fl_x * 2)) * 2\n",
    "            angle_y = math.atan(h / (fl_y * 2)) * 2\n",
    "            fovx = angle_x * 180 / math.pi\n",
    "            fovy = angle_y * 180 / math.pi\n",
    "\n",
    "    print(\n",
    "        f\"camera:\\n\\tres={w,h}\\n\\tcenter={cx,cy}\\n\\tfocal={fl_x,fl_y}\\n\\tfov={fovx,fovy}\\n\\tk={k1,k2} p={p1,p2} \"\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(TEXT_FOLDER, \"images.txt\"), \"r\") as f:\n",
    "        i = 0\n",
    "        bottom = np.array([0.0, 0.0, 0.0, 1.0]).reshape([1, 4])\n",
    "        out = {\n",
    "            \"camera_angle_x\": angle_x,\n",
    "            \"camera_angle_y\": angle_y,\n",
    "            \"fl_x\": fl_x,\n",
    "            \"fl_y\": fl_y,\n",
    "            \"k1\": k1,\n",
    "            \"k2\": k2,\n",
    "            \"p1\": p1,\n",
    "            \"p2\": p2,\n",
    "            \"cx\": cx,\n",
    "            \"cy\": cy,\n",
    "            \"w\": w,\n",
    "            \"h\": h,\n",
    "            \"aabb_scale\": AABB_SCALE,\n",
    "            \"frames\": [],\n",
    "        }\n",
    "\n",
    "        up = np.zeros(3)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            i = i + 1\n",
    "            if i < SKIP_EARLY * 2:\n",
    "                continue\n",
    "            if i % 2 == 1:\n",
    "                elems = line.split(\n",
    "                    \" \"\n",
    "                )  # 1-4 is quat, 5-7 is trans, 9ff is filename (9, if filename contains no spaces)\n",
    "                # name = str(PurePosixPath(Path(IMAGE_FOLDER, elems[9])))\n",
    "                # why is this requireing a relitive path while using ^\n",
    "                image_rel = os.path.relpath(IMAGE_FOLDER)\n",
    "                name = str(f\"./{image_rel}/{'_'.join(elems[9:])}\")\n",
    "                b = sharpness(name)\n",
    "                print(name, \"sharpness=\", b)\n",
    "                image_id = int(elems[0])\n",
    "                qvec = np.array(tuple(map(float, elems[1:5])))\n",
    "                tvec = np.array(tuple(map(float, elems[5:8])))\n",
    "                R = qvec2rotmat(-qvec)\n",
    "                t = tvec.reshape([3, 1])\n",
    "                m = np.concatenate([np.concatenate([R, t], 1), bottom], 0)\n",
    "                c2w = np.linalg.inv(m)\n",
    "                if not args.keep_colmap_coords:\n",
    "                    c2w[0:3, 2] *= -1  # flip the y and z axis\n",
    "                    c2w[0:3, 1] *= -1\n",
    "                    c2w = c2w[[1, 0, 2, 3], :]  # swap y and z\n",
    "                    c2w[2, :] *= -1  # flip whole world upside down\n",
    "\n",
    "                    up += c2w[0:3, 1]\n",
    "\n",
    "                frame = {\"file_path\": name, \"sharpness\": b, \"transform_matrix\": c2w}\n",
    "                out[\"frames\"].append(frame)\n",
    "    nframes = len(out[\"frames\"])\n",
    "\n",
    "    if args.keep_colmap_coords:\n",
    "        flip_mat = np.array([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "        for f in out[\"frames\"]:\n",
    "            f[\"transform_matrix\"] = np.matmul(\n",
    "                f[\"transform_matrix\"], flip_mat\n",
    "            )  # flip cameras (it just works)\n",
    "    else:\n",
    "        # don't keep colmap coords - reorient the scene to be easier to work with\n",
    "\n",
    "        up = up / np.linalg.norm(up)\n",
    "        print(\"up vector was\", up)\n",
    "        R = rotmat(up, [0, 0, 1])  # rotate up vector to [0,0,1]\n",
    "        R = np.pad(R, [0, 1])\n",
    "        R[-1, -1] = 1\n",
    "\n",
    "        for f in out[\"frames\"]:\n",
    "            f[\"transform_matrix\"] = np.matmul(\n",
    "                R, f[\"transform_matrix\"]\n",
    "            )  # rotate up to be the z axis\n",
    "\n",
    "        # find a central point they are all looking at\n",
    "        print(\"computing center of attention...\")\n",
    "        totw = 0.0\n",
    "        totp = np.array([0.0, 0.0, 0.0])\n",
    "        for f in out[\"frames\"]:\n",
    "            mf = f[\"transform_matrix\"][0:3, :]\n",
    "            for g in out[\"frames\"]:\n",
    "                mg = g[\"transform_matrix\"][0:3, :]\n",
    "                p, w = closest_point_2_lines(mf[:, 3], mf[:, 2], mg[:, 3], mg[:, 2])\n",
    "                if w > 0.01:\n",
    "                    totp += p * w\n",
    "                    totw += w\n",
    "        totp /= totw\n",
    "        print(totp)  # the cameras are looking at totp\n",
    "        for f in out[\"frames\"]:\n",
    "            f[\"transform_matrix\"][0:3, 3] -= totp\n",
    "\n",
    "        avglen = 0.0\n",
    "        for f in out[\"frames\"]:\n",
    "            avglen += np.linalg.norm(f[\"transform_matrix\"][0:3, 3])\n",
    "        avglen /= nframes\n",
    "        print(\"avg camera distance from origin\", avglen)\n",
    "        for f in out[\"frames\"]:\n",
    "            f[\"transform_matrix\"][0:3, 3] *= 4.0 / avglen  # scale to \"nerf sized\"\n",
    "\n",
    "    for f in out[\"frames\"]:\n",
    "        f[\"transform_matrix\"] = f[\"transform_matrix\"].tolist()\n",
    "    print(nframes, \"frames\")\n",
    "    print(f\"writing {OUT_PATH}\")\n",
    "    with open(OUT_PATH, \"w\") as outfile:\n",
    "        json.dump(out, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngp training iteration and control API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import commentjson as cjson\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from common import *\n",
    "from scenes import scenes_nerf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyngp as ngp  # noqa\n",
    "\n",
    "\n",
    "def training_step(args):\n",
    "    def control_API(key, value):\n",
    "        nonlocal testbed\n",
    "        if key == \"position\":\n",
    "            testbed.position = value\n",
    "        elif key == \"zoom\":\n",
    "            testbed.zoom = value\n",
    "        elif key == \"render_aabb\":\n",
    "            testbed.render_aabb = ngp.BoundingBox(value[0], value[1])\n",
    "        elif key == \"visualize_cameras\":\n",
    "            testbed.nerf.visualize_cameras = value\n",
    "        elif key == \"background_color\":\n",
    "            testbed.background_color = value\n",
    "        elif key == \"random_bg_color\":\n",
    "            testbed.nerf.training.random_bg_color = value\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def get_render():\n",
    "        nonlocal testbed\n",
    "        return (\n",
    "            testbed.render(args.width, args.height, args.screenshot_spp, True),\n",
    "            testbed.training_step,\n",
    "        )\n",
    "\n",
    "    def get_testbed():\n",
    "        nonlocal testbed\n",
    "        return testbed\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Run neural graphics primitives testbed with additional configuration & output options\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--scene\",\n",
    "        \"--training_data\",\n",
    "        default=\"\",\n",
    "        help=\"The scene to load. Can be the scene's name or a full path to the training data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        default=\"\",\n",
    "        const=\"nerf\",\n",
    "        nargs=\"?\",\n",
    "        choices=[\"nerf\", \"sdf\", \"image\", \"volume\"],\n",
    "        help=\"Mode can be 'nerf', 'sdf', 'image' or 'volume'. Inferred from the scene if unspecified.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--network\",\n",
    "        default=\"\",\n",
    "        help=\"Path to the network config. Uses the scene's default if unspecified.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--load_snapshot\",\n",
    "        default=\"\",\n",
    "        help=\"Load this snapshot before training. recommended extension: .msgpack\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_snapshot\",\n",
    "        default=\"\",\n",
    "        help=\"Save this snapshot after training. recommended extension: .msgpack\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--nerf_compatibility\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Matches parameters with original NeRF. Can cause slowness and worse results on some scenes.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_transforms\",\n",
    "        default=\"\",\n",
    "        help=\"Path to a nerf style transforms json from which we will compute PSNR.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--near_distance\",\n",
    "        default=-1,\n",
    "        type=float,\n",
    "        help=\"Set the distance from the camera at which training rays start for nerf. <0 means use ngp default\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exposure\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=\"Controls the brightness of the image. Positive numbers increase brightness, negative numbers decrease it.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--screenshot_transforms\",\n",
    "        default=\"\",\n",
    "        help=\"Path to a nerf style transforms.json from which to save screenshots.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--screenshot_frames\", nargs=\"*\", help=\"Which frame(s) to take screenshots of.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--screenshot_dir\", default=\"\", help=\"Which directory to output screenshots to.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--screenshot_spp\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"Number of samples per pixel in screenshots.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--video_camera_path\", default=\"\", help=\"The camera path to render.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video_camera_smoothing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Applies additional smoothing to the camera trajectory with the caveat that the endpoint of the camera path may not be reached.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video_fps\", type=int, default=60, help=\"Number of frames per second.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video_n_seconds\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of seconds the rendered video should be long.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video_spp\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of samples per pixel. A larger number means less noise, but slower rendering.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--video_output\",\n",
    "        type=str,\n",
    "        default=\"video.mp4\",\n",
    "        help=\"Filename of the output video.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--save_mesh\",\n",
    "        default=\"\",\n",
    "        help=\"Output a marching-cubes based mesh from the NeRF or SDF model. Supports OBJ and PLY format.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--marching_cubes_res\",\n",
    "        default=256,\n",
    "        type=int,\n",
    "        help=\"Sets the resolution for the marching cubes grid.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--width\",\n",
    "        \"--screenshot_w\",\n",
    "        type=int,\n",
    "        default=1920,\n",
    "        help=\"Resolution width of GUI and screenshots.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--height\",\n",
    "        \"--screenshot_h\",\n",
    "        type=int,\n",
    "        default=1080,\n",
    "        help=\"Resolution height of GUI and screenshots.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--gui\", action=\"store_true\", help=\"Run the testbed GUI interactively.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If the GUI is enabled, controls whether training starts immediately.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_steps\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"Number of steps to train for before quitting.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--sharpen\",\n",
    "        default=0,\n",
    "        help=\"Set amount of sharpening applied to NeRF training images.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args.split())\n",
    "\n",
    "    mode = ngp.TestbedMode.Nerf\n",
    "    configs_dir = os.path.join(ROOT_DIR, \"configs\", \"nerf\")\n",
    "    scenes = scenes_nerf\n",
    "\n",
    "    base_network = os.path.join(configs_dir, \"base.json\")\n",
    "    if args.scene in scenes:\n",
    "        network = (\n",
    "            scenes[args.scene][\"network\"] if \"network\" in scenes[args.scene] else \"base\"\n",
    "        )\n",
    "        base_network = os.path.join(configs_dir, network + \".json\")\n",
    "    network = args.network if args.network else base_network\n",
    "    if not os.path.isabs(network):\n",
    "        network = os.path.join(configs_dir, network)\n",
    "\n",
    "    testbed = ngp.Testbed(mode)\n",
    "    testbed.nerf.sharpen = float(args.sharpen)\n",
    "    testbed.exposure = args.exposure\n",
    "\n",
    "    if args.scene:\n",
    "        scene = args.scene\n",
    "        if not os.path.exists(args.scene) and args.scene in scenes:\n",
    "            scene = os.path.join(\n",
    "                scenes[args.scene][\"data_dir\"], scenes[args.scene][\"dataset\"]\n",
    "            )\n",
    "        testbed.load_training_data(scene)\n",
    "\n",
    "    if args.gui:\n",
    "        # Pick a sensible GUI resolution depending on arguments.\n",
    "        sw = args.width\n",
    "        sh = args.height\n",
    "        while sw * sh > 1920 * 1080 * 4:\n",
    "            sw = int(sw / 2)\n",
    "            sh = int(sh / 2)\n",
    "        testbed.init_window(sw, sh)\n",
    "\n",
    "    if args.load_snapshot:\n",
    "        print(\"Loading snapshot \", args.load_snapshot)\n",
    "        testbed.load_snapshot(args.load_snapshot)\n",
    "    else:\n",
    "        testbed.reload_network_from_file(network)\n",
    "\n",
    "    testbed.shall_train = args.train if args.gui else True\n",
    "\n",
    "    testbed.nerf.render_with_camera_distortion = True\n",
    "\n",
    "    network_stem = os.path.splitext(os.path.basename(network))[0]\n",
    "\n",
    "    if args.near_distance >= 0.0:\n",
    "        print(\"NeRF training ray near_distance \", args.near_distance)\n",
    "        testbed.nerf.training.near_distance = args.near_distance\n",
    "\n",
    "    old_training_step = 0\n",
    "    n_steps = args.n_steps\n",
    "\n",
    "    # If we loaded a snapshot, didn't specify a number of steps, _and_ didn't open a GUI,\n",
    "    # don't train by default and instead assume that the goal is to render screenshots,\n",
    "    # compute PSNR, or render a video.\n",
    "    if n_steps < 0 and (not args.load_snapshot or args.gui):\n",
    "        n_steps = 35000\n",
    "\n",
    "    yield control_API\n",
    "    yield get_render\n",
    "    yield get_testbed\n",
    "\n",
    "    tqdm_last_update = 0\n",
    "    if n_steps > 0:\n",
    "        with tqdm(desc=\"Training\", total=n_steps, unit=\"step\") as t:\n",
    "            while testbed.frame():\n",
    "                if testbed.want_repl():\n",
    "                    repl(testbed)\n",
    "                # What will happen when training is done?\n",
    "                if testbed.training_step >= n_steps:\n",
    "                    if args.gui:\n",
    "                        testbed.shall_train = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Update progress bar\n",
    "                if testbed.training_step < old_training_step or old_training_step == 0:\n",
    "                    old_training_step = 0\n",
    "                    t.reset()\n",
    "\n",
    "                now = time.monotonic()\n",
    "                if now - tqdm_last_update > 0.1:\n",
    "                    t.update(testbed.training_step - old_training_step)\n",
    "                    t.set_postfix(loss=testbed.loss)\n",
    "                    old_training_step = testbed.training_step\n",
    "                    tqdm_last_update = now\n",
    "                    yield t.n\n",
    "\n",
    "    if args.save_snapshot:\n",
    "        print(\"Saving snapshot \", args.save_snapshot)\n",
    "        testbed.save_snapshot(args.save_snapshot, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (colmap convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "import glob\n",
    "import os\n",
    "\n",
    "!rm -r DATA\n",
    "!rm -r result\n",
    "!mkdir DATA\n",
    "!mkdir DATA/images\n",
    "!mkdir DATA/colmap_text\n",
    "!mkdir result\n",
    "\n",
    "if glob.glob(f\"*.mp4\"):\n",
    "    get_frames = 150\n",
    "    FRAMES=os.popen('ffprobe -v error -select_streams v:0 -count_packets -show_entries stream=nb_read_packets -of csv=p=0 *.mp4').read()\n",
    "    FRAMES=int(FRAMES)\n",
    "    FRAMES=set(round((i + 0.499)*FRAMES/get_frames) for i in range(get_frames))\n",
    "    vf_select = \"+\".join(map(lambda X: f\"eq(n\\\\,{X})\", FRAMES))\n",
    "    !ffmpeg -i *.mp4 -vf 'select={vf_select}' -vsync 0 DATA/images/%04d.png\n",
    "\n",
    "elif glob.glob(f\"*.zip\"):\n",
    "    !mkdir original\n",
    "    !mkdir images\n",
    "    !unzip *.zip -d original/\n",
    "    !mv original/**.jpg images\n",
    "    !mv original/**.png images\n",
    "    !mv original/**.JPG images\n",
    "    !mv original/**.PNG images\n",
    "    !mv original/**/*.jpg images\n",
    "    !mv original/**/*.png images\n",
    "    !mv original/**/*.JPG images\n",
    "    !mv original/**/*.PNG images\n",
    "    !for f in images/*\\ *; do mv \"$f\" DATA/\"${f// /_}\"; done\n",
    "    !rm -r original\n",
    "    !rm -r images\n",
    "\n",
    "!cd DATA \\\n",
    " && colmap automatic_reconstructor --random_seed 1010 \\\n",
    "    --workspace_path . \\\n",
    "    --image_path images --mask_path mask_images \\\n",
    "    --data_type video --quality high --camera_model SIMPLE_RADIAL \\\n",
    "    --sparse 1 --dense 0  > /dev/null \\\n",
    " && colmap model_converter --input_path sparse/0 --output_path colmap_text --output_type TXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to nerf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/DATA\n",
    "colmap2nerf(\"--aabb_scale 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/instant-ngp\n",
    "step_iter = iter(\n",
    "    training_step(\n",
    "        \"--train --n_steps 10000 --network base.json --mode nerf --scene /content/DATA \"\n",
    "    )\n",
    ")\n",
    "update_API = next(step_iter)\n",
    "get_render = next(step_iter)\n",
    "get_testbed = next(step_iter)\n",
    "\n",
    "testbed = get_testbed()\n",
    "for _ in step_iter:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rendering result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "x, y, z = np.eye(3)\n",
    "\n",
    "# waypoint\n",
    "# pos = np.asarray([y, -x, -y, z, y, x, -y, -z, y, x, z, -x, -z, x, z])\n",
    "# pos = np.asarray([y, -x, -y, x, y, z, -y, -z, y, x, z, -x, -z, x, z])\n",
    "pos = np.asarray([x, -z, -x, z, x, -z, y, z, -y, -z, y, -x, -y, x, y, -x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruct camera position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    norm = np.linalg.norm(X, ord=2, axis=-1, keepdims=True)\n",
    "    return X / (norm + (norm == 0))\n",
    "\n",
    "\n",
    "def simple_rotation_matrix(A, B):\n",
    "    dim = A.shape[-1]\n",
    "    A_OTH = B - np.einsum(\"ij,ik,ik->ij\", A, A, B)\n",
    "    A, B, A_OTH = map(normalize, (A, B, A_OTH))\n",
    "\n",
    "    prop = np.einsum(\"ij,ik->ijk\", A, A) + np.einsum(\"ij,ik->ijk\", A_OTH, A_OTH)\n",
    "    ind = np.eye(dim, dtype=compute_dtype)[None, ...] - prop\n",
    "    proj_size = np.sqrt(np.square(A_OTH) + np.square(A))\n",
    "    proj = normalize(prop)\n",
    "\n",
    "    δ = (\n",
    "        np.sign(A_OTH) * np.arccos(np.clip(np.einsum(\"ijk,ik->ij\", proj, A), -1, 1))\n",
    "        + np.arccos(np.clip(np.einsum(\"ij,ij->i\", A, B), -1, 1))[:, None]\n",
    "    )\n",
    "\n",
    "    return ind + proj_size[..., None] * (\n",
    "        np.einsum(\"ij,ik->ijk\", np.cos(δ), A)\n",
    "        + np.einsum(\"ij,ik->ijk\", np.sin(δ), A_OTH)\n",
    "    )\n",
    "\n",
    "\n",
    "path = []\n",
    "cam_pos = []\n",
    "\n",
    "# constant matrix\n",
    "z_180 = np.asarray([[-1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "target_pos = np.asarray([0.5, 0.5, 0.5])\n",
    "\n",
    "# configs\n",
    "detail = 4\n",
    "compute_dtype = np.float32\n",
    "theta = np.linspace(0, np.pi / 2, detail * 2 + 1)[1::2]\n",
    "cam_pos = (\n",
    "    np.cos(theta)[:, None] * pos[:-1, None] + np.sin(theta)[:, None] * pos[1:, None]\n",
    ")\n",
    "cam_pos = cam_pos.reshape(-1, 3)\n",
    "\n",
    "direction = np.asarray([[0, 0, 1]] * len(cam_pos))\n",
    "rot_mat = simple_rotation_matrix(direction, -cam_pos)\n",
    "for R, T in zip(rot_mat @ z_180, cam_pos):\n",
    "    path.append(\n",
    "        {\n",
    "            \"R\": (\n",
    "                Rotation.from_matrix(R).as_quat() * np.asarray([1, 1, -1, -1])\n",
    "            ).tolist(),\n",
    "            \"T\": (T + target_pos).tolist(),\n",
    "            \"dof\": 0.0,\n",
    "            \"fov\": 50.625,\n",
    "            \"scale\": 1.5,\n",
    "            \"slice\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "cam_transform = {\"path\": path, \"time\": 1.0}\n",
    "with open(\"/content/spherical_cam.json\", \"w\") as fp:\n",
    "    json.dump(cam_transform, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 10\n",
    "spp = 1\n",
    "exposure = 0\n",
    "video_camera_smoothing = False  # True\n",
    "resolution = [720, 480]  # [1920, 1080]\n",
    "n_frames = len(cam_pos)\n",
    "input = \"/content/spherical_cam.json\"\n",
    "output = \"/content/result/render.mp4\"\n",
    "\n",
    "testbed.load_camera_path(input)\n",
    "testbed.render_aabb = ngp.BoundingBox([-0.5, -0.5, -0.5], [1.5, 1.5, 1.5])\n",
    "if \"tmp\" in os.listdir():\n",
    "    shutil.rmtree(\"tmp\")\n",
    "os.makedirs(\"tmp\")\n",
    "\n",
    "for i in tqdm(\n",
    "    list(range(min(n_frames, n_frames + 1))), unit=\"frames\", desc=f\"Rendering video\"\n",
    "):\n",
    "    testbed.camera_smoothing = video_camera_smoothing and i > 0\n",
    "    frame = testbed.render(\n",
    "        resolution[0],\n",
    "        resolution[1],\n",
    "        spp,\n",
    "        True,\n",
    "        float(i) / n_frames,\n",
    "        float(i + 1) / n_frames,\n",
    "        fps,\n",
    "        shutter_fraction=0.5,\n",
    "    )\n",
    "    write_image(\n",
    "        f\"tmp/{i:04d}.jpg\", np.clip(frame * 2**exposure, 0.0, 1.0), quality=100\n",
    "    )\n",
    "\n",
    "os.system(\n",
    "    f\"ffmpeg -y -framerate {fps} -i tmp/%04d.jpg -c:v libx264 -pix_fmt yuv420p {output}\"\n",
    ")\n",
    "shutil.rmtree(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Video, display\n",
    "\n",
    "# Show results in vscode\n",
    "# display(Video(filename=output, embed=False, html_attributes=\"autoplay loop controls\"))\n",
    "\n",
    "# Save video to ipynb\n",
    "display(Video(filename=output, embed=True, html_attributes=\"autoplay loop controls\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
